{
  "hash": "3cff56db7773884f25e134c9e78c7c5d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Space-Time Prediction of Bike Share Demand: Philadelphia Indego\"\nauthor: Ryan Drake\ndate: 11/30/2025\noutput: \n  html_document:\n    toc: true\n    toc_float: true\n    code_folding: show\n    code_download: true\n---\n\n\n\n# Introduction\n\n## The Rebalancing Challenge in Philadelphia\n\nPhiladelphia's Indego bike share system faces the same operational challenge as every bike share system: **rebalancing bikes to meet anticipated demand**. \n\n## Learning Objectives\n\nThis assignment will be able to:\n\n1. **Understand panel data structure** for space-time analysis\n2. **Create temporal lag variables** to capture demand persistence\n3. **Build multiple predictive models** with increasing complexity\n4. **Validate models temporally** (train on past, test on future)\n5. **Analyze prediction errors** in both space and time\n6. **Engineer new features** based on error patterns\n7. **Critically evaluate** when prediction errors matter most\n\n---\n\n# Setup\n\n## Load Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Core tidyverse\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Spatial data\nlibrary(sf)\nlibrary(tigris)\n\n# Census data\nlibrary(tidycensus)\n\n# Weather data\nlibrary(riem)  # For Philadelphia weather from ASOS stations\n\n# Visualization\nlibrary(viridis)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# here!\nlibrary(here)\n# Get rid of scientific notation. We gotta look good!\noptions(scipen = 999)\n```\n:::\n\n\n## Define Themes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplotTheme <- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),\n  axis.text.y = element_text(size = 10),\n  axis.title = element_text(size = 11, face = \"bold\"),\n  panel.background = element_blank(),\n  panel.grid.major = element_line(colour = \"#D0D0D0\", size = 0.2),\n  panel.grid.minor = element_blank(),\n  axis.ticks = element_blank(),\n  legend.position = \"right\"\n)\n\nmapTheme <- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.line = element_blank(),\n  axis.text = element_blank(),\n  axis.ticks = element_blank(),\n  axis.title = element_blank(),\n  panel.background = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(colour = 'transparent'),\n  panel.grid.minor = element_blank(),\n  legend.position = \"right\",\n  plot.margin = margin(1, 1, 1, 1, 'cm'),\n  legend.key.height = unit(1, \"cm\"),\n  legend.key.width = unit(0.2, \"cm\")\n)\n\npalette5 <- c(\"#eff3ff\", \"#bdd7e7\", \"#6baed6\", \"#3182bd\", \"#08519c\")\n```\n:::\n\n\n## Set Census API Key\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncensus_api_key(\"a55638f442ab081818dbf0ae29a32549253a2ab0\", overwrite = TRUE, install = TRUE)\n```\n:::\n\n\n\n\n---\n\n# Data Import & Preparation\n\n## Load Indego Trip Data (Q3 2025)\n\nQuarter three was chosen to investigate the summer months in comparison to winter months bike share usage. Increased usage and more sporadic usage is expected since people tend to bike more during the summer than the winter - and more for leisure during the summer. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read Q3 2025 data\nindego <- read_csv(\"C:/Users/rydra/OneDrive - PennO365/CPLN 5080 Public Policy/portfolio-setup-rdrake251/Assignments/assignment_05/indego-trips-2025-q3.csv\")\n\n\n# Quick look at the data\nglimpse(indego)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 465,464\nColumns: 15\n$ trip_id             <dbl> 1203073573, 1203073759, 1203073753, 1203073877, 12…\n$ duration            <dbl> 10, 19, 16, 19, 2, 6, 20, 4, 52, 3, 31, 9, 10, 9, …\n$ start_time          <chr> \"7/1/2025 0:06\", \"7/1/2025 0:11\", \"7/1/2025 0:13\",…\n$ end_time            <chr> \"7/1/2025 0:16\", \"7/1/2025 0:30\", \"7/1/2025 0:29\",…\n$ start_station       <dbl> 3163, 3304, 3394, 3207, 3061, 3320, 3201, 3078, 32…\n$ start_lat           <dbl> 39.94974, 39.94234, 39.92400, 39.95441, 39.95425, …\n$ start_lon           <dbl> -75.18097, -75.15399, -75.16959, -75.19200, -75.17…\n$ end_station         <dbl> 3374, 3315, 3394, 3368, 3156, 3320, 3196, 3235, 32…\n$ end_lat             <dbl> 39.97280, 39.94235, 39.92400, 39.97647, 39.95381, …\n$ end_lon             <dbl> -75.17971, -75.21138, -75.16959, -75.17362, -75.17…\n$ bike_id             <chr> \"31674\", \"29473\", \"27889\", \"31825\", \"25400\", \"3172…\n$ plan_duration       <dbl> 30, 1, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,…\n$ trip_route_category <chr> \"One Way\", \"One Way\", \"Round Trip\", \"One Way\", \"On…\n$ passholder_type     <chr> \"Indego30\", \"Walk-up\", \"Indego30\", \"Indego30\", \"In…\n$ bike_type           <chr> \"electric\", \"electric\", \"electric\", \"electric\", \"e…\n```\n\n\n:::\n:::\n\n\n## Examine the Data Structure\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# How many trips?\ncat(\"Total trips in Q3 2025:\", nrow(indego), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTotal trips in Q3 2025: 465464 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Date range\ncat(\"Date range:\", \n    min(mdy_hm(indego$start_time)), \"to\", \n    max(mdy_hm(indego$start_time)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDate range: 1751328360 to 1759276680 \n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique stations?\ncat(\"Unique start stations:\", length(unique(indego$start_station)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnique start stations: 284 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Trip types\ntable(indego$trip_route_category)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   One Way Round Trip \n    434518      30946 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Passholder types\ntable(indego$passholder_type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n Day Pass  Indego30 Indego365      NULL   Walk-up \n    18418    252897    157695         5     36449 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Bike types\ntable(indego$bike_type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nelectric standard \n  299515   165949 \n```\n\n\n:::\n:::\n\n\n## Create Time Bins\n\nWe need to aggregate trips into hourly intervals for our panel data structure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nindego <- indego %>%\n  mutate(\n    # Parse datetime\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    \n    # Create hourly bins\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    \n    # Extract time features\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    \n    # Create useful indicators\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\n# Look at temporal features\nhead(indego %>% select(start_datetime, interval60, week, dotw, hour, weekend))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n  start_datetime      interval60           week dotw   hour weekend\n  <dttm>              <dttm>              <dbl> <ord> <int>   <dbl>\n1 2025-07-01 00:06:00 2025-07-01 00:00:00    26 Tue       0       0\n2 2025-07-01 00:11:00 2025-07-01 00:00:00    26 Tue       0       0\n3 2025-07-01 00:13:00 2025-07-01 00:00:00    26 Tue       0       0\n4 2025-07-01 00:16:00 2025-07-01 00:00:00    26 Tue       0       0\n5 2025-07-01 00:16:00 2025-07-01 00:00:00    26 Tue       0       0\n6 2025-07-01 00:18:00 2025-07-01 00:00:00    26 Tue       0       0\n```\n\n\n:::\n:::\n\n\n---\n\n# Exploratory Analysis\n\n## Trips Over Time\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Daily trip counts\ndaily_trips <- indego %>%\n  group_by(date) %>%\n  summarize(trips = n())\n\nggplot(daily_trips, aes(x = date, y = trips)) +\n  geom_line(color = \"#3182bd\", linewidth = 1) +\n  geom_smooth(se = FALSE, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Indego Daily Ridership - Q3 2025\",\n    subtitle = \"Summer demand patterns in Philadelphia\",\n    x = \"Date\",\n    y = \"Daily Trips\",\n    caption = \"Source: Indego bike share\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](assignment_5_files/figure-html/trips_over_time-1.png){width=672}\n:::\n:::\n\n\n**Question:** What patterns do you see? How does ridership change over time?\n\nRidership increased overall from July to October. I would have thought they are very popular in the summer and ridership would be highest but of course the increase coincides with the school year and move in during late August. It may be that when students, who often don't have cars, need to get around the city they may turn to bike share options. \n\n\n## Hourly Patterns\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Average trips by hour and day type\nhourly_patterns <- indego %>%\n  group_by(hour, weekend) %>%\n  summarize(avg_trips = n() / n_distinct(date)) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(hourly_patterns, aes(x = hour, y = avg_trips, color = day_type)) +\n  geom_line(linewidth = 1.2) +\n  scale_color_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Average Hourly Ridership Patterns\",\n    subtitle = \"Clear commute patterns on weekdays\",\n    x = \"Hour of Day\",\n    y = \"Average Trips per Hour\",\n    color = \"Day Type\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](assignment_5_files/figure-html/hourly_patterns-1.png){width=672}\n:::\n:::\n\n\n**Question:** When are the peak hours? How do weekends differ from weekdays?\n\nThese results are as expected. Weekdays see peak hours during morning and evening rush hours as people are getting to and from work. The weekends have a more gradual rise and fall during the day as there are not generally rush hours for people. Also ridership goes later in the night on the weekend with the weekend night crowds. \n\n## Top Stations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Most popular origin stations\ntop_stations <- indego %>%\n  count(start_station, start_lat, start_lon, name = \"trips\") %>%\n  arrange(desc(trips)) %>%\n  head(20)\n\nkable(top_stations, \n      caption = \"Top 20 Indego Stations by Trip Origins\",\n      format.args = list(big.mark = \",\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Top 20 Indego Stations by Trip Origins</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> start_station </th>\n   <th style=\"text-align:right;\"> start_lat </th>\n   <th style=\"text-align:right;\"> start_lon </th>\n   <th style=\"text-align:right;\"> trips </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 3,010 </td>\n   <td style=\"text-align:right;\"> 39.94711 </td>\n   <td style=\"text-align:right;\"> -75.16618 </td>\n   <td style=\"text-align:right;\"> 7,716 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,032 </td>\n   <td style=\"text-align:right;\"> 39.94527 </td>\n   <td style=\"text-align:right;\"> -75.17971 </td>\n   <td style=\"text-align:right;\"> 5,884 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,213 </td>\n   <td style=\"text-align:right;\"> 39.93887 </td>\n   <td style=\"text-align:right;\"> -75.16663 </td>\n   <td style=\"text-align:right;\"> 5,660 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,163 </td>\n   <td style=\"text-align:right;\"> 39.94974 </td>\n   <td style=\"text-align:right;\"> -75.18097 </td>\n   <td style=\"text-align:right;\"> 5,174 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,359 </td>\n   <td style=\"text-align:right;\"> 39.94888 </td>\n   <td style=\"text-align:right;\"> -75.16978 </td>\n   <td style=\"text-align:right;\"> 4,681 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,185 </td>\n   <td style=\"text-align:right;\"> 39.95169 </td>\n   <td style=\"text-align:right;\"> -75.15888 </td>\n   <td style=\"text-align:right;\"> 4,670 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,028 </td>\n   <td style=\"text-align:right;\"> 39.94061 </td>\n   <td style=\"text-align:right;\"> -75.14958 </td>\n   <td style=\"text-align:right;\"> 4,607 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,020 </td>\n   <td style=\"text-align:right;\"> 39.94855 </td>\n   <td style=\"text-align:right;\"> -75.19007 </td>\n   <td style=\"text-align:right;\"> 4,556 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,022 </td>\n   <td style=\"text-align:right;\"> 39.95472 </td>\n   <td style=\"text-align:right;\"> -75.18323 </td>\n   <td style=\"text-align:right;\"> 4,475 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,066 </td>\n   <td style=\"text-align:right;\"> 39.94561 </td>\n   <td style=\"text-align:right;\"> -75.17348 </td>\n   <td style=\"text-align:right;\"> 4,353 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,059 </td>\n   <td style=\"text-align:right;\"> 39.96244 </td>\n   <td style=\"text-align:right;\"> -75.16121 </td>\n   <td style=\"text-align:right;\"> 4,265 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,063 </td>\n   <td style=\"text-align:right;\"> 39.94633 </td>\n   <td style=\"text-align:right;\"> -75.16980 </td>\n   <td style=\"text-align:right;\"> 4,230 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,161 </td>\n   <td style=\"text-align:right;\"> 39.95486 </td>\n   <td style=\"text-align:right;\"> -75.18091 </td>\n   <td style=\"text-align:right;\"> 4,223 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,101 </td>\n   <td style=\"text-align:right;\"> 39.94295 </td>\n   <td style=\"text-align:right;\"> -75.15955 </td>\n   <td style=\"text-align:right;\"> 4,183 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,054 </td>\n   <td style=\"text-align:right;\"> 39.96250 </td>\n   <td style=\"text-align:right;\"> -75.17420 </td>\n   <td style=\"text-align:right;\"> 4,117 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,030 </td>\n   <td style=\"text-align:right;\"> 39.93935 </td>\n   <td style=\"text-align:right;\"> -75.15716 </td>\n   <td style=\"text-align:right;\"> 3,986 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,362 </td>\n   <td style=\"text-align:right;\"> 39.94816 </td>\n   <td style=\"text-align:right;\"> -75.16226 </td>\n   <td style=\"text-align:right;\"> 3,965 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,061 </td>\n   <td style=\"text-align:right;\"> 39.95425 </td>\n   <td style=\"text-align:right;\"> -75.17761 </td>\n   <td style=\"text-align:right;\"> 3,716 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,057 </td>\n   <td style=\"text-align:right;\"> 39.96439 </td>\n   <td style=\"text-align:right;\"> -75.17987 </td>\n   <td style=\"text-align:right;\"> 3,668 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,296 </td>\n   <td style=\"text-align:right;\"> 39.95134 </td>\n   <td style=\"text-align:right;\"> -75.16758 </td>\n   <td style=\"text-align:right;\"> 3,665 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n---\n\n# Get Philadelphia Spatial Context\n\n## Load Philadelphia Census Data\n\nWe'll get census tract data to add demographic context to our stations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get Philadelphia census tracts\nphilly_census <- get_acs(\n  geography = \"tract\",\n  variables = c(\n    \"B01003_001\",  # Total population\n    \"B19013_001\",  # Median household income\n    \"B08301_001\",  # Total commuters\n    \"B08301_010\",  # Commute by transit\n    \"B02001_002\",  # White alone\n    \"B25077_001\"   # Median home value\n  ),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2022,\n  geometry = TRUE,\n  output = \"wide\"\n) %>%\n  rename(\n    Total_Pop = B01003_001E,\n    Med_Inc = B19013_001E,\n    Total_Commuters = B08301_001E,\n    Transit_Commuters = B08301_010E,\n    White_Pop = B02001_002E,\n    Med_Home_Value = B25077_001E\n  ) %>%\n  mutate(\n    Percent_Taking_Transit = (Transit_Commuters / Total_Commuters) * 100,\n    Percent_White = (White_Pop / Total_Pop) * 100\n  ) %>%\n  st_transform(crs = 4326)  # WGS84 for lat/lon matching\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=====================================                                 |  54%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |========================================                              |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check the data\nglimpse(philly_census)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 408\nColumns: 17\n$ GEOID                  <chr> \"42101001500\", \"42101001800\", \"42101002802\", \"4…\n$ NAME                   <chr> \"Census Tract 15; Philadelphia County; Pennsylv…\n$ Total_Pop              <dbl> 3251, 3300, 5720, 4029, 4415, 1815, 3374, 2729,…\n$ B01003_001M            <dbl> 677, 369, 796, 437, 853, 210, 480, 734, 763, 11…\n$ Med_Inc                <dbl> 110859, 114063, 78871, 61583, 32347, 48581, 597…\n$ B19013_001M            <dbl> 24975, 30714, 20396, 22293, 4840, 13812, 6278, …\n$ Total_Commuters        <dbl> 2073, 2255, 3032, 2326, 1980, 969, 2427, 708, 2…\n$ B08301_001M            <dbl> 387, 308, 478, 383, 456, 189, 380, 281, 456, 68…\n$ Transit_Commuters      <dbl> 429, 123, 685, 506, 534, 192, 658, 218, 438, 51…\n$ B08301_010M            <dbl> 188, 66, 219, 144, 285, 71, 278, 184, 176, 235,…\n$ White_Pop              <dbl> 2185, 2494, 3691, 3223, 182, 984, 2111, 231, 35…\n$ B02001_002M            <dbl> 268, 381, 592, 380, 88, 190, 463, 112, 238, 778…\n$ Med_Home_Value         <dbl> 568300, 605000, 350600, 296400, 76600, 289700, …\n$ B25077_001M            <dbl> 58894, 34876, 12572, 22333, 10843, 118720, 1506…\n$ geometry               <MULTIPOLYGON [°]> MULTIPOLYGON (((-75.16558 3..., MU…\n$ Percent_Taking_Transit <dbl> 20.694645, 5.454545, 22.592348, 21.754084, 26.9…\n$ Percent_White          <dbl> 67.2100892, 75.5757576, 64.5279720, 79.9950360,…\n```\n\n\n:::\n:::\n\n\n## Map Philadelphia Context\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Map median income\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = NA) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Context for understanding bike share demand patterns\"\n  ) +\n  # Stations \n  geom_point(\n    data = indego,\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 0.25, alpha = 0.6\n  ) +\n  mapTheme\n```\n\n::: {.cell-output-display}\n![](assignment_5_files/figure-html/map_philly-1.png){width=672}\n:::\n:::\n\n\n## Join Census Data to Stations\n\nWe'll spatially join census characteristics to each bike station.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create sf object for stations\nstations_sf <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Spatial join to get census tract for each station\nstations_census <- st_join(stations_sf, philly_census, left = TRUE) %>%\n  st_drop_geometry()\n\n# Look at the result - investigate whether all of the stations joined to census data -- according to the map above there are stations in non-residential tracts.\n\nstations_for_map <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  left_join(\n    stations_census %>% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %>%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Add back to trip data\nindego_census <- indego %>%\n  left_join(\n    stations_census %>% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n\n\n# Prepare data for visualization\nstations_for_map <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  left_join(\n    stations_census %>% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %>%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Create the map showing problem stations\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = \"white\", size = 0.1) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar,\n    na.value = \"grey90\"\n  ) +\n  # Stations with census data (small grey dots)\n  geom_point(\n    data = stations_for_map %>% filter(has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"grey30\", size = 1, alpha = 0.6\n  ) +\n  # Stations WITHOUT census data (red X marks the spot)\n  geom_point(\n    data = stations_for_map %>% filter(!has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 1, shape = 4, stroke = 1.5\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Indego stations shown (RED = no census data match)\",\n    caption = \"Red X marks indicate stations that didn't join to census tracts\"\n  ) +\n  mapTheme\n```\n\n::: {.cell-output-display}\n![](assignment_5_files/figure-html/join_census_to_stations-1.png){width=672}\n:::\n:::\n\n\n# Dealing with missing data\n\nWe need to decide what to do with the non-residential bike share stations. For this example, we are going to remove them -- this is not necessarily the right way to do things always, but for the sake of simplicity, we are narrowing our scope to only stations in residential neighborhoods. We might opt to create a separate model for non-residential stations..\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Identify which stations to keep\nvalid_stations <- stations_census %>%\n  filter(!is.na(Med_Inc)) %>%\n  pull(start_station)\n\n# Filter trip data to valid stations only\nindego_census <- indego %>%\n  filter(start_station %in% valid_stations) %>%\n  left_join(\n    stations_census %>% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n```\n:::\n\n\n\n# Get Weather Data\n\nWeather significantly affects bike share demand! Let's get hourly weather for Philadelphia.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get weather from Philadelphia International Airport (KPHL)\n# This covers Q1 2025: January 1 - March 31\nweather_data <- riem_measures(\n  station = \"PHL\",  # Philadelphia International Airport\n  date_start = \"2025-07-01\",\n  date_end = \"2025-09-30\"\n)\n\n# Process weather data\nweather_processed <- weather_data %>%\n  mutate(\n    interval60 = floor_date(valid, unit = \"hour\"),\n    Temperature = tmpf,  # Temperature in Fahrenheit\n    Feel = feel, #Feels like temperature in Fahrenheit\n    Precipitation = ifelse(is.na(p01i), 0, p01i),  # Hourly precip in inches\n    Wind_Speed = sknt  # Wind speed in knots\n  ) %>%\n  select(interval60, Temperature, Precipitation, Feel, Wind_Speed) %>%\n  distinct()\n\n# Check for missing hours and interpolate if needed\nweather_complete <- weather_processed %>%\n  complete(interval60 = seq(min(interval60), max(interval60), by = \"hour\")) %>%\n  fill(Temperature, Precipitation, Feel, Wind_Speed, .direction = \"down\")\n\n# Look at the weather\nsummary(weather_complete %>% select(Temperature, Precipitation, Feel, Wind_Speed))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Temperature    Precipitation           Feel          Wind_Speed    \n Min.   :57.00   Min.   :0.000000   Min.   : 57.00   Min.   : 0.000  \n 1st Qu.:70.00   1st Qu.:0.000000   1st Qu.: 70.00   1st Qu.: 4.000  \n Median :76.00   Median :0.000000   Median : 76.00   Median : 6.000  \n Mean   :75.73   Mean   :0.008013   Mean   : 76.96   Mean   : 6.421  \n 3rd Qu.:81.00   3rd Qu.:0.000000   3rd Qu.: 82.32   3rd Qu.: 9.000  \n Max.   :98.00   Max.   :1.390000   Max.   :108.59   Max.   :29.000  \n```\n\n\n:::\n:::\n\n\n## Visualize Weather Patterns\n\nWho is ready for a Philly winter?!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(weather_complete, aes(x = interval60, y = Temperature)) +\n  geom_line(color = \"#3182bd\", alpha = 0.7) +\n  geom_smooth(se = FALSE, color = \"red\") +\n  labs(\n    title = \"Philadelphia Temperature - Q3 2025\",\n    subtitle = \"Summer to early fall transition\",\n    x = \"Date\",\n    y = \"Temperature (°F)\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](assignment_5_files/figure-html/visualize_weather-1.png){width=672}\n:::\n:::\n\n\n---\n\n# Create Space-Time Panel\n\n## Aggregate Trips to Station-Hour Level\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Count trips by station-hour\ntrips_panel <- indego_census %>%\n  group_by(interval60, start_station, start_lat, start_lon,\n           Med_Inc, Percent_Taking_Transit, Percent_White, Total_Pop) %>%\n  summarize(Trip_Count = n()) %>%\n  ungroup()\n\n# How many station-hour observations?\nnrow(trips_panel)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 214178\n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique stations?\nlength(unique(trips_panel$start_station))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 263\n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique hours?\nlength(unique(trips_panel$interval60))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2208\n```\n\n\n:::\n:::\n\n\n## Create Complete Panel Structure\n\nNot every station has trips every hour. We need a **complete panel** where every station-hour combination exists (even if Trip_Count = 0).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate expected panel size\nn_stations <- length(unique(trips_panel$start_station))\nn_hours <- length(unique(trips_panel$interval60))\nexpected_rows <- n_stations * n_hours\n\ncat(\"Expected panel rows:\", format(expected_rows, big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nExpected panel rows: 580,704 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Current rows:\", format(nrow(trips_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCurrent rows: 214,178 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Missing rows:\", format(expected_rows - nrow(trips_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMissing rows: 366,526 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Create complete panel\nstudy_panel <- expand.grid(\n  interval60 = unique(trips_panel$interval60),\n  start_station = unique(trips_panel$start_station)\n) %>%\n  # Join trip counts\n  left_join(trips_panel, by = c(\"interval60\", \"start_station\")) %>%\n  # Replace NA trip counts with 0\n  mutate(Trip_Count = replace_na(Trip_Count, 0))\n\n# Fill in station attributes (they're the same for all hours)\nstation_attributes <- trips_panel %>%\n  group_by(start_station) %>%\n  summarize(\n    start_lat = first(start_lat),\n    start_lon = first(start_lon),\n    Med_Inc = first(Med_Inc),\n    Percent_Taking_Transit = first(Percent_Taking_Transit),\n    Percent_White = first(Percent_White),\n    Total_Pop = first(Total_Pop)\n  )\n\nstudy_panel <- study_panel %>%\n  left_join(station_attributes, by = \"start_station\")\n\n# Verify we have complete panel\ncat(\"Complete panel rows:\", format(nrow(study_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nComplete panel rows: 580,704 \n```\n\n\n:::\n:::\n\n\n## Add Time Features\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel <- study_panel %>%\n  mutate(\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n```\n:::\n\n\n## Join Weather Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel <- study_panel %>%\n  left_join(weather_complete, by = \"interval60\")\n\n# Check for missing values\nsummary(study_panel %>% select(Trip_Count, Temperature, Precipitation, Feel))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Trip_Count       Temperature    Precipitation        Feel       \n Min.   : 0.0000   Min.   :57.00   Min.   :0.000   Min.   : 57.00  \n 1st Qu.: 0.0000   1st Qu.:70.00   1st Qu.:0.000   1st Qu.: 70.00  \n Median : 0.0000   Median :76.00   Median :0.000   Median : 76.00  \n Mean   : 0.7236   Mean   :75.73   Mean   :0.008   Mean   : 76.96  \n 3rd Qu.: 1.0000   3rd Qu.:81.00   3rd Qu.:0.000   3rd Qu.: 82.32  \n Max.   :22.0000   Max.   :98.00   Max.   :1.390   Max.   :108.59  \n                   NA's   :6312    NA's   :6312    NA's   :6312    \n```\n\n\n:::\n:::\n\n\n---\n\n# Create Temporal Lag Variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sort by station and time\nstudy_panel <- study_panel %>%\n  arrange(start_station, interval60)\n\n# Create lag variables WITHIN each station\nstudy_panel <- study_panel %>%\n  group_by(start_station) %>%\n  mutate(\n    lag1Hour = lag(Trip_Count, 1),\n    lag2Hours = lag(Trip_Count, 2),\n    lag3Hours = lag(Trip_Count, 3),\n    lag12Hours = lag(Trip_Count, 12),\n    lag1day = lag(Trip_Count, 24),\n    lag1week = lag(Trip_Count, 168),\n  ) %>%\n  ungroup()\n\n# Remove rows with NA lags (first 24 hours for each station)\nstudy_panel_complete <- study_panel %>%\n  filter(!is.na(lag1week))\n\ncat(\"Rows after removing NA lags:\", format(nrow(study_panel_complete), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows after removing NA lags: 671,439 \n```\n\n\n:::\n:::\n\n\n## Visualize Lag Correlations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample one station to visualize\nexample_station <- study_panel_complete %>%\n  filter(start_station == first(start_station)) %>%\n  head(168)  # One week\n\n# Plot actual vs lagged demand\nggplot(example_station, aes(x = interval60)) +\n  geom_line(aes(y = Trip_Count, color = \"Current\"), linewidth = 1) +\n  geom_line(aes(y = lag1Hour, color = \"1 Hour Ago\"), linewidth = 1, alpha = 0.7) +\n  geom_line(aes(y = lag1day, color = \"24 Hours Ago\"), linewidth = 1, alpha = 0.7) +\n  scale_color_manual(values = c(\n    \"Current\" = \"#08519c\",\n    \"1 Hour Ago\" = \"#3182bd\",\n    \"24 Hours Ago\" = \"#6baed6\"\n  )) +\n  labs(\n    title = \"Temporal Lag Patterns at One Station\",\n    subtitle = \"Past demand predicts future demand\",\n    x = \"Date-Time\",\n    y = \"Trip Count\",\n    color = \"Time Period\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](assignment_5_files/figure-html/lag_correlations-1.png){width=672}\n:::\n:::\n\n\n---\n\n# Temporal Train/Test Split\n\n**CRITICAL:** We must train on PAST data and test on FUTURE data!\n\n## Why Temporal Validation Matters\n\nIn real operations, at 6:00 AM on March 15, we need to predict demand for March 15-31. We have data from Jan 1 - March 14, but NOT from March 15-31 (it hasn't happened yet!).\n\n**Wrong approach:** Train on weeks 10-13, test on weeks 1-9 (predicting past from future!)\n\n**Correct approach:** Train on weeks 1-9, test on weeks 10-13 (predicting future from past)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split by week\n# Q3 has weeks 27-39 (July-Sep)\n# Train on weeks 27-36 (July 1 - early Sep)\n# Test on weeks 36-39 (rest of Sep)\n\n# Which stations have trips in BOTH early and late periods?\nearly_stations <- study_panel_complete %>%\n  filter(week < 36) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\nlate_stations <- study_panel_complete %>%\n  filter(week >= 36) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\n# Keep only stations that appear in BOTH periods\ncommon_stations <- intersect(early_stations, late_stations)\n\n\n# Filter panel to only common stations\nstudy_panel_complete <- study_panel_complete %>%\n  filter(start_station %in% common_stations)\n\n# NOW create train/test split\ntrain <- study_panel_complete %>%\n  filter(week < 36)\n\ntest <- study_panel_complete %>%\n  filter(week >= 36)\n\ncat(\"Training observations:\", format(nrow(train), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining observations: 464,195 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing observations:\", format(nrow(test), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting observations: 207,244 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Training date range:\", min(train$date), \"to\", max(train$date), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining date range: 20274 to 20333 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing date range:\", min(test$date), \"to\", max(test$date), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting date range: 20334 to 20361 \n```\n\n\n:::\n:::\n\n\n---\n\n# Build Predictive Models\n\nWe'll build 5 models with increasing complexity to see what improves predictions.\n\n## Model 1: Baseline (Time + Weather)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create day of week factor with treatment (dummy) coding\ntrain <- train %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(train$dotw_simple) <- contr.treatment(7)\n\n# Now run the model\nmodel1 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation,\n  data = train\n)\n\nsummary(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.7488 -0.7597 -0.2079  0.2120 21.1050 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)        0.3682491  0.0270944  13.591 < 0.0000000000000002 ***\nas.factor(hour)1  -0.0524469  0.0126346  -4.151     0.00003309870185 ***\nas.factor(hour)2  -0.1258408  0.0127281  -9.887 < 0.0000000000000002 ***\nas.factor(hour)3  -0.1768582  0.0127195 -13.905 < 0.0000000000000002 ***\nas.factor(hour)4  -0.1658679  0.0125123 -13.256 < 0.0000000000000002 ***\nas.factor(hour)5  -0.0547020  0.0128611  -4.253     0.00002106887679 ***\nas.factor(hour)6   0.1873902  0.0125605  14.919 < 0.0000000000000002 ***\nas.factor(hour)7   0.4560745  0.0125870  36.234 < 0.0000000000000002 ***\nas.factor(hour)8   0.9416129  0.0125198  75.210 < 0.0000000000000002 ***\nas.factor(hour)9   0.6276052  0.0127047  49.400 < 0.0000000000000002 ***\nas.factor(hour)10  0.4834604  0.0125652  38.476 < 0.0000000000000002 ***\nas.factor(hour)11  0.5281827  0.0121913  43.325 < 0.0000000000000002 ***\nas.factor(hour)12  0.6158844  0.0123947  49.689 < 0.0000000000000002 ***\nas.factor(hour)13  0.6891155  0.0125768  54.792 < 0.0000000000000002 ***\nas.factor(hour)14  0.7559300  0.0124246  60.841 < 0.0000000000000002 ***\nas.factor(hour)15  0.8648970  0.0125051  69.163 < 0.0000000000000002 ***\nas.factor(hour)16  1.0831084  0.0126702  85.485 < 0.0000000000000002 ***\nas.factor(hour)17  1.4375105  0.0128842 111.572 < 0.0000000000000002 ***\nas.factor(hour)18  1.0852029  0.0128307  84.579 < 0.0000000000000002 ***\nas.factor(hour)19  0.8735245  0.0129826  67.284 < 0.0000000000000002 ***\nas.factor(hour)20  0.6011154  0.0127982  46.969 < 0.0000000000000002 ***\nas.factor(hour)21  0.4227890  0.0125658  33.646 < 0.0000000000000002 ***\nas.factor(hour)22  0.2879695  0.0124415  23.146 < 0.0000000000000002 ***\nas.factor(hour)23  0.1280527  0.0124667  10.272 < 0.0000000000000002 ***\ndotw_simple2       0.1042901  0.0066564  15.668 < 0.0000000000000002 ***\ndotw_simple3       0.0366454  0.0067196   5.454     0.00000004940690 ***\ndotw_simple4       0.0458157  0.0066479   6.892     0.00000000000552 ***\ndotw_simple5       0.0989077  0.0070119  14.106 < 0.0000000000000002 ***\ndotw_simple6       0.0041096  0.0069015   0.595                0.552    \ndotw_simple7      -0.0648442  0.0066994  -9.679 < 0.0000000000000002 ***\nTemperature       -0.0021649  0.0003221  -6.721     0.00000000001810 ***\nPrecipitation     -0.3599307  0.0270714 -13.296 < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 464163 degrees of freedom\nMultiple R-squared:  0.1089,\tAdjusted R-squared:  0.1088 \nF-statistic:  1830 on 31 and 464163 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\nThe model uses Monday as the baseline. Each coefficient represents the difference \nin expected trips per station-hour compared to Monday - dow_simple2 = Tuesday.\n\n**Weekday Pattern (Tue-Fri):**\n\n- All weekdays have positive coefficients (0.026 to 0.088)\n- Friday has the highest weekday effect (+0.088)\n- Weekdays likely benefit from concentrated commuting patterns\n\n**Weekend Pattern (Sat-Sun):**\n\n- Only Sunday weekend days have a negative coefficient (-0.065) while Saturday has a positive (+0.0064)\n\n**Hourly Interpretation**\n\nHour   Coefficient   Interpretation\n0      (baseline)    0.000 trips/hour (midnight)\n1      -0.054       slightly fewer than midnight\n...\n6      +0.184       morning activity starting\n7      +0.450       morning rush building\n8      +0.937       PEAK morning rush\n9      +0.614       post-rush\n...\n17     +1.401       PEAK evening rush (5 PM)\n18     +1.058       evening declining\n...\n23     +0.131       late night minimal\n\n\n\n## Model 2: Add Temporal Lags\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day,\n  data = train\n)\n\nsummary(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.0652 -0.4805 -0.1328  0.1699 17.4097 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)        0.1517212  0.0231242   6.561      0.0000000000534 ***\nas.factor(hour)1   0.0050328  0.0107803   0.467              0.64060    \nas.factor(hour)2  -0.0138561  0.0108633  -1.276              0.20213    \nas.factor(hour)3  -0.0321651  0.0108597  -2.962              0.00306 ** \nas.factor(hour)4  -0.0004616  0.0106885  -0.043              0.96555    \nas.factor(hour)5   0.0967690  0.0109891   8.806 < 0.0000000000000002 ***\nas.factor(hour)6   0.2704594  0.0107360  25.192 < 0.0000000000000002 ***\nas.factor(hour)7   0.4132746  0.0107637  38.395 < 0.0000000000000002 ***\nas.factor(hour)8   0.7128398  0.0107183  66.507 < 0.0000000000000002 ***\nas.factor(hour)9   0.2635703  0.0108823  24.220 < 0.0000000000000002 ***\nas.factor(hour)10  0.1961770  0.0107428  18.261 < 0.0000000000000002 ***\nas.factor(hour)11  0.2828768  0.0104193  27.149 < 0.0000000000000002 ***\nas.factor(hour)12  0.3467520  0.0105967  32.723 < 0.0000000000000002 ***\nas.factor(hour)13  0.3904312  0.0107558  36.300 < 0.0000000000000002 ***\nas.factor(hour)14  0.4453933  0.0106272  41.911 < 0.0000000000000002 ***\nas.factor(hour)15  0.5177799  0.0107026  48.379 < 0.0000000000000002 ***\nas.factor(hour)16  0.6693837  0.0108560  61.660 < 0.0000000000000002 ***\nas.factor(hour)17  0.9030493  0.0110698  81.578 < 0.0000000000000002 ***\nas.factor(hour)18  0.4893097  0.0110539  44.266 < 0.0000000000000002 ***\nas.factor(hour)19  0.3743646  0.0111577  33.552 < 0.0000000000000002 ***\nas.factor(hour)20  0.1886039  0.0109998  17.146 < 0.0000000000000002 ***\nas.factor(hour)21  0.1666560  0.0107593  15.489 < 0.0000000000000002 ***\nas.factor(hour)22  0.1298422  0.0106320  12.212 < 0.0000000000000002 ***\nas.factor(hour)23  0.0638527  0.0106388   6.002      0.0000000019524 ***\ndotw_simple2       0.0385025  0.0056813   6.777      0.0000000000123 ***\ndotw_simple3      -0.0040894  0.0057344  -0.713              0.47577    \ndotw_simple4       0.0183779  0.0056725   3.240              0.00120 ** \ndotw_simple5       0.0191991  0.0059886   3.206              0.00135 ** \ndotw_simple6      -0.0282515  0.0058924  -4.795      0.0000016307921 ***\ndotw_simple7      -0.0488761  0.0057166  -8.550 < 0.0000000000000002 ***\nTemperature       -0.0026041  0.0002749  -9.472 < 0.0000000000000002 ***\nPrecipitation     -0.0924507  0.0231301  -3.997      0.0000641644538 ***\nlag1Hour           0.3997495  0.0013675 292.320 < 0.0000000000000002 ***\nlag3Hours          0.1256638  0.0013447  93.451 < 0.0000000000000002 ***\nlag1day            0.1416324  0.0012445 113.804 < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.056 on 464160 degrees of freedom\nMultiple R-squared:  0.3514,\tAdjusted R-squared:  0.3514 \nF-statistic:  7397 on 34 and 464160 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\n**Question:** Did adding lags improve R²? Why or why not?\n\nYes, adding lag variables greatly improved the R² from 0.1062 to 0.3526. This is because ridership is strongly auto correlated meaning that the number of trips in a given hour depend on trips that happened previously - bike demand is temporal and also patterned and cyclical (rush hours, time of year like students coming back in August, and adding time is less noisy than the weather variable). \n\n## Model 3: Add Demographics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel3 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y,\n  data = train\n)\n\nsummary(model3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day + Med_Inc.x + \n    Percent_Taking_Transit.y + Percent_White.y, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7333 -0.7395 -0.2690  0.4612 17.0509 \n\nCoefficients:\n                              Estimate    Std. Error t value\n(Intercept)               0.9124583540  0.0518050679  17.613\nas.factor(hour)1          0.0908102512  0.0372008493   2.441\nas.factor(hour)2         -0.0168983180  0.0412507019  -0.410\nas.factor(hour)3         -0.1090948792  0.0484549671  -2.251\nas.factor(hour)4         -0.0824845255  0.0441874329  -1.867\nas.factor(hour)5          0.0483567208  0.0348432205   1.388\nas.factor(hour)6          0.2395443810  0.0296970726   8.066\nas.factor(hour)7          0.3614287436  0.0281316543  12.848\nas.factor(hour)8          0.7185249130  0.0270794670  26.534\nas.factor(hour)9          0.0973848673  0.0275035768   3.541\nas.factor(hour)10         0.0565382169  0.0276687465   2.043\nas.factor(hour)11         0.1408814346  0.0271324155   5.192\nas.factor(hour)12         0.2279907560  0.0272051962   8.380\nas.factor(hour)13         0.2727282649  0.0272704651  10.001\nas.factor(hour)14         0.3221692226  0.0269756701  11.943\nas.factor(hour)15         0.4004098876  0.0269313573  14.868\nas.factor(hour)16         0.6365484724  0.0269780592  23.595\nas.factor(hour)17         0.9343347246  0.0269879403  34.620\nas.factor(hour)18         0.4204094079  0.0272368975  15.435\nas.factor(hour)19         0.2963802784  0.0276344611  10.725\nas.factor(hour)20         0.0958521108  0.0280349078   3.419\nas.factor(hour)21         0.1075895157  0.0283460496   3.796\nas.factor(hour)22         0.1008011517  0.0290615724   3.469\nas.factor(hour)23         0.0226518792  0.0304195344   0.745\ndotw_simple2              0.0711711700  0.0118514838   6.005\ndotw_simple3             -0.0158120091  0.0120194501  -1.316\ndotw_simple4              0.0303529206  0.0119195442   2.546\ndotw_simple5              0.0131942165  0.0123166648   1.071\ndotw_simple6              0.0280243274  0.0123672199   2.266\ndotw_simple7             -0.0180702320  0.0122886648  -1.470\nTemperature              -0.0008759491  0.0005530565  -1.584\nPrecipitation            -0.7625965365  0.0745475135 -10.230\nlag1Hour                  0.2997553139  0.0021213871 141.302\nlag3Hours                 0.0894704552  0.0021875613  40.900\nlag1day                   0.1130869851  0.0020583035  54.942\nMed_Inc.x                 0.0000008997  0.0000001155   7.790\nPercent_Taking_Transit.y -0.0041528216  0.0004234566  -9.807\nPercent_White.y           0.0025077079  0.0002063528  12.153\n                                     Pr(>|t|)    \n(Intercept)              < 0.0000000000000002 ***\nas.factor(hour)1                     0.014644 *  \nas.factor(hour)2                     0.682064    \nas.factor(hour)3                     0.024357 *  \nas.factor(hour)4                     0.061946 .  \nas.factor(hour)5                     0.165188    \nas.factor(hour)6          0.00000000000000073 ***\nas.factor(hour)7         < 0.0000000000000002 ***\nas.factor(hour)8         < 0.0000000000000002 ***\nas.factor(hour)9                     0.000399 ***\nas.factor(hour)10                    0.041015 *  \nas.factor(hour)11         0.00000020788321907 ***\nas.factor(hour)12        < 0.0000000000000002 ***\nas.factor(hour)13        < 0.0000000000000002 ***\nas.factor(hour)14        < 0.0000000000000002 ***\nas.factor(hour)15        < 0.0000000000000002 ***\nas.factor(hour)16        < 0.0000000000000002 ***\nas.factor(hour)17        < 0.0000000000000002 ***\nas.factor(hour)18        < 0.0000000000000002 ***\nas.factor(hour)19        < 0.0000000000000002 ***\nas.factor(hour)20                    0.000629 ***\nas.factor(hour)21                    0.000147 ***\nas.factor(hour)22                    0.000523 ***\nas.factor(hour)23                    0.456485    \ndotw_simple2              0.00000000191429276 ***\ndotw_simple3                         0.188332    \ndotw_simple4                         0.010882 *  \ndotw_simple5                         0.284059    \ndotw_simple6                         0.023452 *  \ndotw_simple7                         0.141434    \nTemperature                          0.113234    \nPrecipitation            < 0.0000000000000002 ***\nlag1Hour                 < 0.0000000000000002 ***\nlag3Hours                < 0.0000000000000002 ***\nlag1day                  < 0.0000000000000002 ***\nMed_Inc.x                 0.00000000000000675 ***\nPercent_Taking_Transit.y < 0.0000000000000002 ***\nPercent_White.y          < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.313 on 165327 degrees of freedom\n  (298830 observations deleted due to missingness)\nMultiple R-squared:  0.2464,\tAdjusted R-squared:  0.2462 \nF-statistic:  1461 on 37 and 165327 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n**Question:** Did demographics improve R²?\n\nNo, demographics decreased the R² from 0.3526 to 0.247. This was an NA data issue where around two thirds of the data set was dropped from the training set. The temporal autocorrelation still dominates the model so demographics don't add much to the predictive value of the model. \n\n\n## Model 4: Add Station Fixed Effects\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel4 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station),\n  data = train\n)\n\n# Summary too long with all station dummies, just show key metrics\ncat(\"Model 4 R-squared:\", summary(model4)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 4 R-squared: 0.2761157 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model 4 Adj R-squared:\", summary(model4)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 4 Adj R-squared: 0.2748176 \n```\n\n\n:::\n:::\n\n\n**What do station fixed effects capture?** Baseline differences in demand across stations.\n\nThey remove the noise of all time invariable characteristics like access, land-use context, neighborhood density, proximity to amenities etc. This controls for these factors so the model can just compare within-station variation over time. \n\n## Model 5: Add Rush Hour Interaction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel5 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + rush_hour + as.factor(month) +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station) +\n    rush_hour * weekend,  # Rush hour effects different on weekends\n  data = train\n)\n\ncat(\"Model 5 R-squared:\", summary(model5)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 5 R-squared: 0.2815065 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model 5 Adj R-squared:\", summary(model5)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 5 Adj R-squared: 0.2802051 \n```\n\n\n:::\n:::\n\n\n---\n\n# Model Evaluation\n\n## Calculate Predictions and MAE\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get predictions on test set\n\n# Create day of week factor with treatment (dummy) coding\ntest <- test %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(test$dotw_simple) <- contr.treatment(7)\n\ntest <- test %>%\n  mutate(\n    pred1 = predict(model1, newdata = test),\n    pred2 = predict(model2, newdata = test),\n    pred3 = predict(model3, newdata = test),\n    pred4 = predict(model4, newdata = test),\n    pred5 = predict(model5, newdata = test)\n  )\n\n# Calculate MAE for each model\nmae_results <- data.frame(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE)\n  )\n)\n\nkable(mae_results, \n      digits = 2,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Mean Absolute Error by Model (Test Set)</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model </th>\n   <th style=\"text-align:right;\"> MAE (trips) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 1. Time + Weather </td>\n   <td style=\"text-align:right;\"> 0.84 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2. + Temporal Lags </td>\n   <td style=\"text-align:right;\"> 0.71 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 3. + Demographics </td>\n   <td style=\"text-align:right;\"> 0.97 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 4. + Station FE </td>\n   <td style=\"text-align:right;\"> 0.95 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 5. + Rush Hour Interaction </td>\n   <td style=\"text-align:right;\"> 0.96 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Visualize Model Comparison\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mae_results, aes(x = reorder(Model, -MAE), y = MAE)) +\n  geom_col(fill = \"#3182bd\", alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 2)), vjust = -0.5) +\n  labs(\n    title = \"Model Performance Comparison\",\n    subtitle = \"Lower MAE = Better Predictions\",\n    x = \"Model\",\n    y = \"Mean Absolute Error (trips)\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](assignment_5_files/figure-html/compare_models-1.png){width=672}\n:::\n:::\n\n\n**Question:** Which features gave us the biggest improvement?\n\nThe second model: temporal lag gave the best performance, lowest MAE, at 0.71 because bike share demand is temporal and this model best captures this dependance. \n\n---\n\n# Space-Time Error Analysis\n\n## Observed vs. Predicted\n\nLet's use our best model (Model 2) for error analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest <- test %>%\n  mutate(\n    error = Trip_Count - pred2,\n    abs_error = abs(error),\n    time_of_day = case_when(\n      hour < 7 ~ \"Overnight\",\n      hour >= 7 & hour < 10 ~ \"AM Rush\",\n      hour >= 10 & hour < 15 ~ \"Mid-Day\",\n      hour >= 15 & hour <= 18 ~ \"PM Rush\",\n      hour > 18 ~ \"Evening\"\n    )\n  )\n\n# Scatter plot by time and day type\nggplot(test, aes(x = Trip_Count, y = pred2)) +\n  geom_point(alpha = 0.2, color = \"#3182bd\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkgreen\") +\n  facet_grid(weekend ~ time_of_day) +\n  labs(\n    title = \"Observed vs. Predicted Bike Trips\",\n    subtitle = \"Model 2 performance by time period\",\n    x = \"Observed Trips\",\n    y = \"Predicted Trips\",\n    caption = \"Red line = perfect predictions; Green line = actual model fit\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](assignment_5_files/figure-html/obs_vs_pred-1.png){width=672}\n:::\n:::\n\n\n**Question:** Where is the model performing well? Where is it struggling?\n\nThe model really only performs well at stations with low volume demand (such as below 5 observed trips). Unfortunately, the model under predicts high demand periods like rush hour and high volume stations (those that are greater than 5 or so observed trips). This could be due to the model missing non-linear dynamics that impact ridership. \n\n## Spatial Error Patterns\n\nAre prediction errors clustered in certain parts of Philadelphia?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate MAE by station\nstation_errors <- test %>%\n  group_by(start_station, start_lat.x, start_lon.y) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n## Create Two Maps Side-by-Side with Proper Legends (sorry these maps are ugly)\n\n# Calculate station errors\nstation_errors <- test %>%\n  filter(!is.na(pred2)) %>%\n  group_by(start_station, start_lat.x, start_lon.y) %>%\n  summarize(\n    MAE = mean(abs(Trip_Count - pred2), na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n# Map 1: Prediction Errors\np1 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon, y = start_lat, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE\\n(trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),  # Fewer, cleaner breaks\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\",\n       subtitle = \"Higher in Center City\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 1.5,\n    barheight = 12,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand\np2 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg\\nDemand\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),  # Clear breaks\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\",\n       subtitle = \"Trips per station-hour\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 1.5,\n    barheight = 12,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n\n\n\n# Map 1: Prediction Errors\np1 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE (trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand  \np2 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg Demand (trips/hour)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Combine\ngrid.arrange(\n  p1, p2,\n  ncol = 2\n  )\n```\n\n::: {.cell-output-display}\n![](assignment_5_files/figure-html/spatial_errors-1.png){width=672}\n:::\n\n```{.r .cell-code}\np1\n```\n\n::: {.cell-output-display}\n![](assignment_5_files/figure-html/spatial_errors-2.png){width=672}\n:::\n:::\n\n\n**Question:** Do you see spatial clustering of errors? What neighborhoods have high errors?\n\nThe areas such as center city and university city have the highest demand and also highest MAE. This is expected based on the model performance where it cannot account for large volume stations or other non-linear aspects of the problem. For example, 30th street station or bike shares around the universities may have different peak or surge behaviors from other variables aside from what we used in the model like train arrival times or large university events etc. \n\n## Temporal Error Patterns\n\nWhen are we most wrong?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# MAE by time of day and day type\ntemporal_errors <- test %>%\n  group_by(time_of_day, weekend) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(temporal_errors, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Prediction Errors by Time Period\",\n    subtitle = \"When is the model struggling most?\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](assignment_5_files/figure-html/temporal_errors-1.png){width=672}\n:::\n:::\n\n\n## Errors and Demographics\n\nAre prediction errors related to neighborhood characteristics?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Join demographic data to station errors\nstation_errors_demo <- station_errors %>%\n  left_join(\n    station_attributes %>% select(start_station, Med_Inc, Percent_Taking_Transit, Percent_White),\n    by = \"start_station\"\n  ) %>%\n  filter(!is.na(Med_Inc))\n\n# Create plots\np1 <- ggplot(station_errors_demo, aes(x = Med_Inc, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_x_continuous(labels = scales::dollar) +\n  labs(title = \"Errors vs. Median Income\", x = \"Median Income\", y = \"MAE\") +\n  plotTheme\n\np2 <- ggplot(station_errors_demo, aes(x = Percent_Taking_Transit, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Transit Usage\", x = \"% Taking Transit\", y = \"MAE\") +\n  plotTheme\n\np3 <- ggplot(station_errors_demo, aes(x = Percent_White, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Race\", x = \"% White\", y = \"MAE\") +\n  plotTheme\n\ngrid.arrange(p1, p2, p3, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](assignment_5_files/figure-html/errors_demographics-1.png){width=672}\n:::\n:::\n\n\n**Critical Question:** Are prediction errors systematically higher in certain demographic groups? What are the equity implications?\n\nThese prediction errors are consistent with the spatial clustering seen.The model performs best in transit-heavy, often lower-income/majority-minority neighborhoods, and worse in affluent/Whiter neighborhoods with irregular trip patterns. \n\n---\n\n**Compare results** to Q1 2025 (Winter):\n\n   - How do MAE values compare? Why might they differ?\n   \n   The MAE in Q1 for Model 2 was 0.50 compared to Q3 Model 3 of 0.71. The summer months are higher demand seasons versus the winter months. This makes predicting summer bike patterns harder because of the higher variance leading to higher MAE.\n   \n   - Are temporal patterns different (e.g., summer vs. winter)?\n   \n   Summer and Winter are two very different seasons. Weather is a portion of why summer months are harder to predict as well. Rainstorms and other weather are more sporadic and with higher ridership can disrupt the predictive pattern more easily. Winter is cold everyday, assuming people still riding in the winter are consistent with this aspect, the weather patterns are less interruptive. There are also midday increases in ridership during the summer which could be tourists riding around the city. \n   \n   - Which features are most important in your quarter?\n   \n   Again, distance to universities and center city (as seen in the prediction errors maps) spike more in the summer months than in winter months could be helpful. Weather is actually the most important feature (or one of them) as heat and rain coupled with the higher ridership makes it harder to predict summer patters over winter patterns. \n   \n   \n## Part 3: Feature Engineering & model improvement: Adding 2-3 new features to improve the model:\n\n## Add Feature 1: same hour last week\n\nAdded code to lines 581 chunk to add the lag1week variable:\n\n[lag1week = lag(Trip_Count, 168)] and [filter(!is.na(lag1week))]\n\n## Model 6: Add same hour one week lag\n\nThis variable was chosen based on the predictive value of the temporal lag models. It could potentially improve the model to look back even further in time to predict the future. This was true but only incrementally. The R2 slightly increased to 0.3578 from 0.3513. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel6 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation + lag1week + lag1Hour + lag3Hours + lag1day,\n  data = train\n)\n\nsummary(model6)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1week + lag1Hour + lag3Hours + lag1day, \n    data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8958 -0.4845 -0.1391  0.1704 17.4782 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)        0.0855497  0.0230243   3.716             0.000203 ***\nas.factor(hour)1   0.0009293  0.0107247   0.087             0.930947    \nas.factor(hour)2  -0.0218567  0.0108077  -2.022             0.043144 *  \nas.factor(hour)3  -0.0436470  0.0108048  -4.040  0.00005355257608722 ***\nas.factor(hour)4  -0.0129719  0.0106348  -1.220             0.222557    \nas.factor(hour)5   0.0796475  0.0109351   7.284  0.00000000000032546 ***\nas.factor(hour)6   0.2605243  0.0106815  24.390 < 0.0000000000000002 ***\nas.factor(hour)7   0.4130357  0.0107081  38.572 < 0.0000000000000002 ***\nas.factor(hour)8   0.7197802  0.0106634  67.500 < 0.0000000000000002 ***\nas.factor(hour)9   0.2789224  0.0108283  25.759 < 0.0000000000000002 ***\nas.factor(hour)10  0.2140525  0.0106904  20.023 < 0.0000000000000002 ***\nas.factor(hour)11  0.2981352  0.0103678  28.756 < 0.0000000000000002 ***\nas.factor(hour)12  0.3494794  0.0105420  33.151 < 0.0000000000000002 ***\nas.factor(hour)13  0.3843570  0.0107006  35.919 < 0.0000000000000002 ***\nas.factor(hour)14  0.4351710  0.0105733  41.157 < 0.0000000000000002 ***\nas.factor(hour)15  0.5117392  0.0106476  48.061 < 0.0000000000000002 ***\nas.factor(hour)16  0.6646401  0.0108001  61.540 < 0.0000000000000002 ***\nas.factor(hour)17  0.9059036  0.0110127  82.260 < 0.0000000000000002 ***\nas.factor(hour)18  0.4937868  0.0109969  44.902 < 0.0000000000000002 ***\nas.factor(hour)19  0.3804739  0.0111004  34.276 < 0.0000000000000002 ***\nas.factor(hour)20  0.1939857  0.0109432  17.727 < 0.0000000000000002 ***\nas.factor(hour)21  0.1705066  0.0107039  15.929 < 0.0000000000000002 ***\nas.factor(hour)22  0.1346820  0.0105773  12.733 < 0.0000000000000002 ***\nas.factor(hour)23  0.0660513  0.0105839   6.241  0.00000000043592416 ***\ndotw_simple2       0.0360465  0.0056520   6.378  0.00000000018004396 ***\ndotw_simple3      -0.0033892  0.0057048  -0.594             0.552446    \ndotw_simple4       0.0236186  0.0056437   4.185  0.00002852853883764 ***\ndotw_simple5       0.0290188  0.0059593   4.870  0.00000111914468811 ***\ndotw_simple6      -0.0180855  0.0058638  -3.084             0.002041 ** \ndotw_simple7      -0.0444085  0.0056874  -7.808  0.00000000000000582 ***\nTemperature       -0.0022464  0.0002735  -8.212 < 0.0000000000000002 ***\nPrecipitation     -0.1206957  0.0230142  -5.244  0.00000015685361078 ***\nlag1week           0.0873478  0.0012562  69.536 < 0.0000000000000002 ***\nlag1Hour           0.3873746  0.0013720 282.336 < 0.0000000000000002 ***\nlag3Hours          0.1135726  0.0013490  84.190 < 0.0000000000000002 ***\nlag1day            0.1312334  0.0012471 105.231 < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.05 on 464159 degrees of freedom\nMultiple R-squared:  0.3581,\tAdjusted R-squared:  0.3581 \nF-statistic:  7398 on 35 and 464159 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n## Add Feature 2: \"feels-like\" weather \n\nThis feature was selected because of the heat aspect of summer months increasing variability and decreasing prediction value. Added Feel variable for feels like temperature in chunk 439. This didn't change the R2 at all. The Feel variable had very little predictive influence even if it was significant. \n\n## Model 7: Add \"feels-like\" weather \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel7 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation + Feel + lag1week + lag1Hour + lag3Hours + lag1day,\n  data = train\n)\n\nsummary(model7)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + Feel + lag1week + lag1Hour + lag3Hours + \n    lag1day, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8912 -0.4844 -0.1390  0.1687 17.4738 \n\nCoefficients:\n                     Estimate  Std. Error t value             Pr(>|t|)    \n(Intercept)        0.02006073  0.02762163   0.726              0.46767    \nas.factor(hour)1  -0.00002092  0.01072682  -0.002              0.99844    \nas.factor(hour)2  -0.02219227  0.01080782  -2.053              0.04004 *  \nas.factor(hour)3  -0.04496122  0.01080894  -4.160   0.0000318817014825 ***\nas.factor(hour)4  -0.01425428  0.01063880  -1.340              0.18030    \nas.factor(hour)5   0.07925602  0.01093531   7.248   0.0000000000004245 ***\nas.factor(hour)6   0.25999822  0.01068202  24.340 < 0.0000000000000002 ***\nas.factor(hour)7   0.41311368  0.01070793  38.580 < 0.0000000000000002 ***\nas.factor(hour)8   0.71986062  0.01066318  67.509 < 0.0000000000000002 ***\nas.factor(hour)9   0.27921512  0.01082832  25.786 < 0.0000000000000002 ***\nas.factor(hour)10  0.21462128  0.01069101  20.075 < 0.0000000000000002 ***\nas.factor(hour)11  0.29870886  0.01036850  28.809 < 0.0000000000000002 ***\nas.factor(hour)12  0.35000087  0.01054254  33.199 < 0.0000000000000002 ***\nas.factor(hour)13  0.38466370  0.01070061  35.948 < 0.0000000000000002 ***\nas.factor(hour)14  0.43577598  0.01057409  41.212 < 0.0000000000000002 ***\nas.factor(hour)15  0.51211343  0.01064778  48.096 < 0.0000000000000002 ***\nas.factor(hour)16  0.66332595  0.01080428  61.395 < 0.0000000000000002 ***\nas.factor(hour)17  0.90376877  0.01102372  81.984 < 0.0000000000000002 ***\nas.factor(hour)18  0.49151142  0.01100951  44.644 < 0.0000000000000002 ***\nas.factor(hour)19  0.37656336  0.01113754  33.810 < 0.0000000000000002 ***\nas.factor(hour)20  0.19145239  0.01095894  17.470 < 0.0000000000000002 ***\nas.factor(hour)21  0.16790069  0.01072088  15.661 < 0.0000000000000002 ***\nas.factor(hour)22  0.13316625  0.01058302  12.583 < 0.0000000000000002 ***\nas.factor(hour)23  0.06641658  0.01058407   6.275   0.0000000003496090 ***\ndotw_simple2       0.03674567  0.00565428   6.499   0.0000000000810770 ***\ndotw_simple3      -0.00122939  0.00572687  -0.215              0.83002    \ndotw_simple4       0.02690932  0.00569545   4.725   0.0000023051338008 ***\ndotw_simple5       0.03036320  0.00596741   5.088   0.0000003616785514 ***\ndotw_simple6      -0.01801058  0.00586369  -3.072              0.00213 ** \ndotw_simple7      -0.04317578  0.00569459  -7.582   0.0000000000000341 ***\nTemperature        0.00158774  0.00093432   1.699              0.08925 .  \nPrecipitation     -0.12331679  0.02302185  -5.357   0.0000000848840438 ***\nFeel              -0.00292645  0.00068189  -4.292   0.0000177368666740 ***\nlag1week           0.08769128  0.00125868  69.669 < 0.0000000000000002 ***\nlag1Hour           0.38726884  0.00137223 282.219 < 0.0000000000000002 ***\nlag3Hours          0.11341801  0.00134946  84.047 < 0.0000000000000002 ***\nlag1day            0.13076172  0.00125191 104.450 < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.05 on 464158 degrees of freedom\nMultiple R-squared:  0.3581,\tAdjusted R-squared:  0.3581 \nF-statistic:  7194 on 36 and 464158 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n## Model 7: Predictive Errors\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create day of week factor with treatment (dummy) coding\ntest <- test %>%\n  mutate(dotw_simple = factor(dotw, \n                              levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\ncontrasts(test$dotw_simple) <- contr.treatment(7)\n\n# Generate predictions\ntest$pred7 <- predict(model7, newdata = test)\n\n# Calculate MAE for Model 7\nmae_model7 <- mean(abs(test$Trip_Count - test$pred7), na.rm = TRUE)\n\ncat(\"Model 7 MAE:\", round(mae_model7, 4), \"trips\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 7 MAE: 0.7117 trips\n```\n\n\n:::\n\n```{.r .cell-code}\nmae_results <- mae_results %>%\n  add_row(Model = \"7. + Weekly Lag + Feel\", MAE = mae_model7)\n```\n:::\n\n\n## Part 4: Critical Reflection \n\nWrite 1-2 paragraphs addressing:\n\n1. **Operational implications:**\n   - Is your final MAE \"good enough\" for Indego to use?\n   \n   My final MAE for model 7 was 0.712. This shapes overall demand still. This may be \"good enough\" since it is off less than a bike per hour which is better than naive baselines. Though good enough for Indego would be dependent on their goals. This model is really only good for steady, low-volume demand stations. \n   \n   - When do prediction errors cause problems for re balancing?\n   \n   The errors come with stations with heavy use, sporadic weather like summer rain fall, heat waves, distance to amenities like universities or center city which have more non-linear usage like large events, or during rush hours. \n   \n   - Would you recommend deploying this system? Under what conditions?\n   \n   This system could be helpful in re balancing from and over all stand point. However, there are some conditions in which it is not useful as stated above. There would still need to be real-time demand monitoring, adding weather forecasts like time lags into the prediction would be interesting?\n\n2. **Equity considerations:**\n\n   - Do prediction errors disproportionately affect certain neighborhoods?\n   \n   Yes, however it is largely wealthy/whiter neighborhoods that this model poorly predicts. Instead it does a better job of predicting low-volume/lower-income neighborhoods. Demand is more stable in low-volume areas than in higher, more volatile areas. \n  \n   - Could this system worsen existing disparities in bike access?\n   \n   Yes, this model, without safeguards, could worsen existing disparities. Low volume/low income areas could also be a reflection of poor service not just low demand. The existing service seen in the locations map doesn't represent many lower income neighborhoods already. Predicting demand alone could just reinforce this structure. These models should only be used as guidance. \n   \n    - What safeguards would you recommend?\n      \n   Check in's on predictive capacity by race and income etc. should be done regularly. Also, the city could institute minimum service guarantees that should be encoded and applied to Indego/the city for servicing the city overall.     \n   \n\n3. **Model limitations:**\n\n   - What patterns is your model missing?\n   \n   Although the model functions and is \"good enough\" for use, it still does not encompass non-linear aspects like distance to universities and center city which have nonlinear event patterns and crowd patterns or weather events that are more sporadic throughout the seasons, like summer rain events. \n   \n   - What assumptions might not hold in real deployment?\n   \n   The lag week assumption is interesting to add since it improved model performance, yet weekly weather can drastically different than day by day weather. Also, station demand is connected and the model doesn't account for this. It treats each station individually as an assumption instead. \n   \n   - How would you improve this with more time/data?\n   \n   I would add more information to the model like holidays, large events from universities or center city like the convention center, station capacities, or even try more machine learning techniques that can go beyond non-linear modelling. \n\n---\n\nknitr::convert_chunk_header(\"assignment_5.Rmd\", \"assignment_5.qmd\")\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}