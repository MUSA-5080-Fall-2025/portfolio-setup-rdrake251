#combine 2023 & 2024 crime datasets
crime <- rbind(crime_2023, crime_2024)
#create centroids for schools dataset
schools <- if (any(st_geometry_type(schools_polygons) %in% c("POLYGON","MULTIPOLYGON"))) {
st_centroid(schools_polygons, )
} else {
schools_polygons
}
#crop transit data to philadelphia
philly_boundary <- st_union(nhoods)
transit <- st_filter(transit, philly_boundary, .predicate = st_within)
# Transit stops (raw)
ggplot() +
geom_sf(data = nhoods,
fill = NA, color = "grey70", linewidth = 0.3) +
geom_sf(data = transit, size = 0.3, alpha = 0.6) +
labs(title = "Raw Layer Check: Transit Stops (SEPTA Spring 2025)") +
theme_void()
# Hospitals (raw)
ggplot() +
geom_sf(data = nhoods,
fill = NA, color = "grey70", linewidth = 0.3) +
geom_sf(data = hospitals, size = 0.6, alpha = 0.7) +
labs(title = "Raw Layer Check: Hospitals") +
theme_void()
# Parks & Recreation Program Sites (raw)
ggplot() +
geom_sf(data = nhoods,
fill = NA, color = "grey70", linewidth = 0.3) +
geom_sf(data = parksrec, size = 0.4, alpha = 0.6) +
labs(title = "Raw Layer Check: Parks & Recreation Sites") +
theme_void()
# Schools (centroids of polygons) — raw
ggplot() +
geom_sf(data = nhoods,
fill = NA, color = "grey70", linewidth = 0.3) +
geom_sf(data = schools, size = 0.4, alpha = 0.7) +
labs(title = "Raw Layer Check: Schools (Centroids)") +
theme_void()
# Crime points are huge; sampling for speed
set.seed(5080)
crime_quick <- if (nrow(crime) > 30000) dplyr::slice_sample(crime, n = 30000) else crime
ggplot() +
geom_sf(data = nhoods,
fill = NA, color = "grey70", linewidth = 0.3) +
geom_sf(data = crime_quick, size = 0.1, alpha = 0.25) +
labs(title = "Raw Layer Check: Crime Incidents (sampled if large)") +
theme_void()
#| message: false
#| warning: false
#clean sales data
sales_xy <- st_coordinates(OPA_points)
ok_sales  <- complete.cases(sales_xy)
OPA_points <- OPA_points[ok_sales, ]    # keep only rows with valid XY
sales_xy  <- st_coordinates(OPA_points) # refresh coordinates
transit_xy <- st_coordinates(transit)
hosp_xy    <- st_coordinates(hospitals)
# feature 1 - distance to nearest transit stop (ft)
knn_tr <- FNN::get.knnx(
data  = st_coordinates(transit),
query = sales_xy,
k = 1)
OPA_points <- OPA_points %>%
mutate(dist_nearest_transit_ft = as.numeric(knn_tr$nn.dist[,1]))
# feature 2 - distance to nearest hospital (ft)
knn_hp <- FNN::get.knnx(
data  = st_coordinates(hospitals),
query = sales_xy,
k = 1)
OPA_points <- OPA_points %>%
mutate(dist_nearest_hospital_ft = as.numeric(knn_hp$nn.dist[,1]))
#| message: false
#| warning: false
# feature 3 - parks/rec sites within 0.25 mi (count)
rel_parks <- sf::st_is_within_distance(OPA_points, parksrec, dist = r_park_ft)
OPA_points <- OPA_points %>%
mutate(parks_cnt_0p25mi = lengths(rel_parks))
# feature 4 - crime count within 0.5 mi (per square mile)
crime_buffer <- st_buffer(OPA_points, dist = r_crime_ft)
rel_crime <- st_intersects(crime_buffer, crime, sparse = TRUE)
# count number of crimes
crime_cnt <- lengths(rel_crime)
rm(rel_crime)
OPA_points <- OPA_points |>
mutate(
crime_cnt_0p5mi     = crime_cnt,
log1p_crime_cnt_0p5 = log1p(crime_cnt_0p5mi)
)
# feature 5 - schools within 0.5 mi (using centroids)
rel_sch <- sf::st_is_within_distance(OPA_points, schools, dist = r_school_ft)
OPA_points <- OPA_points %>%
mutate(schools_cnt_0p5mi = lengths(rel_sch))
## Transit Accessibility
ggplot(OPA_points, aes(x = dist_nearest_transit_ft)) +
geom_histogram(fill = "steelblue", color = "white", bins = 30) +
labs(title = "Distribution: Distance to Nearest Transit Stop",
x = "Feet to Nearest Stop", y = "Count") +
theme_minimal()
ggplot(OPA_points) +
geom_sf(data = nhoods,
fill = NA, color = "grey70", linewidth = 0.3) +
geom_sf(aes(color = dist_nearest_transit_ft), size = 0.5) +
scale_color_viridis_c(option = "plasma", labels = comma) +
labs(title = "Transit Accessibility Across Sales Parcels",
color = "Distance (ft)") +
theme_void()
## Hospital Proximity
ggplot(OPA_points, aes(x = dist_nearest_hospital_ft)) +
geom_histogram(fill = "darkorange", color = "white", bins = 30) +
labs(title = "Distribution: Distance to Nearest Hospital",
x = "Feet to Nearest Hospital", y = "Count") +
theme_minimal()
ggplot(OPA_points) +
geom_sf(data = nhoods,
fill = NA, color = "grey70", linewidth = 0.3) +
geom_sf(aes(color = dist_nearest_hospital_ft), size = 0.5) +
scale_color_viridis_c(option = "magma", labels = comma) +
labs(title = "Hospital Accessibility Across Sales Parcels",
color = "Distance (ft)") +
theme_void()
## Parks & Recreation
ggplot(OPA_points, aes(x = parks_cnt_0p25mi)) +
geom_histogram(fill = "seagreen", color = "white", bins = 20) +
labs(title = "Distribution: Parks & Rec Sites Within 0.25 mi",
x = "Count within 0.25 mi", y = "Number of Parcels") +
theme_minimal()
ggplot(OPA_points) +
geom_sf(data = nhoods,
fill = NA, color = "grey70", linewidth = 0.3) +
geom_sf(aes(color = parks_cnt_0p25mi), size = 0.6) +
scale_color_viridis_c(option = "viridis") +
labs(title = "Proximity to Parks & Recreation (0.25 mi Buffer)",
color = "Parks Count") +
theme_void()
## Crime Counts
ggplot(OPA_points, aes(x = crime_cnt_0p5mi)) +
geom_histogram(fill = "firebrick", color = "white", bins = 30) +
labs(title = "Distribution: Crime Incidents Within 0.5 mi",
x = "Crime Count (2023–2024)", y = "Number of Parcels") +
theme_minimal()
ggplot(OPA_points) +
geom_sf(data = nhoods,
fill = NA, color = "grey70", linewidth = 0.3) +
geom_sf(aes(color = log1p_crime_cnt_0p5), size = 0.6) +
scale_color_viridis_c(option = "inferno") +
labs(title = "Crime Exposure (log-transformed within 0.5 mi)",
color = "log(1+Crime Count)") +
theme_void()
## Schools Accessibility
ggplot(OPA_points, aes(x = schools_cnt_0p5mi)) +
geom_histogram(fill = "purple", color = "white", bins = 20) +
labs(title = "Distribution: Schools Within 0.5 mi",
x = "School Count (0.5 mi Buffer)", y = "Number of Parcels") +
theme_minimal()
ggplot(OPA_points) +
geom_sf(data = nhoods,
fill = NA, color = "grey70", linewidth = 0.3) +
geom_sf(aes(color = schools_cnt_0p5mi), size = 0.6) +
scale_color_viridis_c(option = "cividis") +
labs(title = "School Accessibility (0.5 mi Buffer)",
color = "Schools Count") +
theme_void()
sp_data <- st_read("./data/OPA_data.geojson", quiet = T)
str(sp_data$sale_price)
sp_data_filtered <- sp_data %>%
mutate(sale_price = as.numeric(sale_price)) %>%
filter(sale_price > 1000)
ggplot(sp_data_filtered, aes(x = sale_price)) +
geom_histogram(
binwidth = 20000,
fill = "grey",
color = "black"
) +
labs(
title = "Histogram of Sale Prices in Philadelphia",
x = "Sale Price",
y = "Count of Homes"
) +
theme_minimal() +
scale_x_continuous(labels = label_dollar()) +
coord_cartesian(xlim = c(0, 2000000), ylim = c(0, 3000))
summary(sp_data_filtered$sale_price)
sp_data_filtered <- sp_data_filtered %>%
mutate(
sale_price = as.numeric(sale_price),
sale_price_capped = pmin(sale_price, quantile(sale_price, 0.99, na.rm = TRUE))
)
ggplot(sp_data_filtered) +
geom_sf(aes(color = sale_price_capped), size = 0.6, alpha = 0.7) +
scale_color_viridis_c(labels = label_dollar(), name = "Sale Price (USD)") +
labs(title = "Philadelphia Sale Prices") +
theme_minimal()
# function to check for statistically significant correlations between independent variables
sig_corr <- function(dat, dep_var) {
# remove the independent variable from the dataset
dat_corr <- dat %>% select(-all_of(dep_var))
# run a correlation matrix for the independent vars
correlation_matrix <- rcorr(as.matrix(dat_corr))
values <- correlation_matrix$r
vifs <- apply(values, 1, function(x){
return(round(1/(1-abs(x)), 2))
})
values_df <- values %>% as.data.frame()
vifs_df <- vifs %>% as.data.frame()
# convert correlation coefficients and p-values to long format
corCoeff_df <- correlation_matrix$r %>%
as.data.frame() %>%
mutate(var1 = rownames(.))
corVIF_df <- vifs %>%
as.data.frame() %>%
mutate(var1 = rownames(.))
corPval_df <- correlation_matrix$P %>%
as.data.frame() %>%
mutate(var1 = rownames(.))
# merge long format data
corMerge <- list(
corCoeff_df %>% pivot_longer(-var1, names_to = "var2", values_to = "correlation") %>% as.data.frame(),
corVIF_df %>% pivot_longer(-var1, names_to = "var2", values_to = "vif_factor") %>% as.data.frame(),
corPval_df %>% pivot_longer(-var1, names_to = "var2", values_to = "p_value") %>% as.data.frame()) %>%
reduce(left_join, by = c("var1", "var2"))
# filter to isolate unique pairs, then rows with correlation > 0.5 and p < 0.05
corUnfiltered <- corMerge %>%
filter(var1 != var2) %>%
rowwise() %>%
filter(var1 < var2) %>%
ungroup() %>%
as.data.frame()
corFiltered <- corUnfiltered %>%
filter(abs(vif_factor) > 3 & p_value < 0.05) %>%
arrange(desc(abs(correlation)))
# save the raw correlation values and the filtered variable pairs
final <- set_names(list(values_df, vifs_df, corUnfiltered, corFiltered),
c("R2", "VIF", "AllCor", "SelCor"))
return(final)
}
# create a dataset with just modeling variables
OPA_modelvars <- OPA_points %>% select(sale_price, total_livable_area, building_age, number_of_bedrooms, number_of_bathrooms,
pop_totE, med_hh_incE, med_ageE,
dist_nearest_transit_ft, dist_nearest_hospital_ft, parks_cnt_0p25mi, log1p_crime_cnt_0p5, schools_cnt_0p5mi,
)
# calculate VIFs and determine potentially troublesome correlations between variables
vif_check <- sig_corr(OPA_modelvars %>% st_drop_geometry(), dep_var = "sale_price")
kable(vif_check[["VIF"]])
model1_data <- na.omit(OPA_points)
model1 <- lm(
sale_price ~
total_livable_area +
number_of_bedrooms +
number_of_bathrooms +
building_age,
data = model1_data
)
summary(model1)
model2_data <- na.omit(OPA_points)
model2 <- lm(
sale_price ~
total_livable_area +
number_of_bedrooms +
number_of_bathrooms +
building_age +
pop_totE +
med_hh_incE +
med_ageE,
data = model2_data
)
summary(model2)
model3_data <- na.omit(OPA_points)
model3 <- lm(
sale_price ~
total_livable_area +
number_of_bedrooms +
number_of_bathrooms +
building_age +
total_area +
pop_totE +
med_hh_incE +
med_ageE +
dist_nearest_transit_ft +
dist_nearest_hospital_ft +
parks_cnt_0p25mi +
log1p_crime_cnt_0p5,
data = model3_data
)
summary(model3)
# join data separately here to avoid conflicts with earlier code blocks
OPA_points_copy <- left_join(OPA_points,
neighbor_points %>%
select(parcel_number, wealthy_neighborhood) %>%
st_drop_geometry(),
by = "parcel_number")
model4_data <- na.omit(OPA_points_copy)
model4 <- lm(
sale_price ~
total_livable_area +
number_of_bedrooms +
number_of_bathrooms +
building_age +
total_area +
pop_totE +
med_hh_incE +
med_ageE +
dist_nearest_transit_ft +
dist_nearest_hospital_ft +
parks_cnt_0p25mi +
log1p_crime_cnt_0p5 +
number_of_bathrooms * wealthy_neighborhood +
int_type_tarea +
int_value_larea +
int_value_tarea +
int_larea_econd +
int_larea_icond +
int_larea_beds,
data = model4_data
)
summary(model4)
#there is only a premium on wealth neighborhood for total area, total livable area, and number of bathrooms that are significant. There is also a significant value for int_value_larea just from interacting the OPA data itsself which just assesses market value and size scalability.
# Define 10-fold CV
train_control <- trainControl(
method = "cv",
number = 10,
savePredictions = "final"
)
# Model 1: Structural Features Only
model1_cv <- train(
sale_price ~
total_livable_area +
number_of_bedrooms +
number_of_bathrooms +
building_age,
data = na.omit(OPA_points),
method = "lm",
trControl = train_control
)
model1_cv
# Model 2: Structural + Census
model2_cv <- train(
sale_price ~
total_livable_area +
number_of_bedrooms +
number_of_bathrooms +
building_age +
pop_totE +
med_hh_incE +
med_ageE,
data = na.omit(OPA_points),
method = "lm",
trControl = train_control
)
model2_cv
# Model 3: Structural + Census + Spatial
model3_cv <- train(
sale_price ~
total_livable_area +
number_of_bedrooms +
number_of_bathrooms +
building_age +
total_area +
pop_totE +
med_hh_incE +
med_ageE +
dist_nearest_transit_ft +
dist_nearest_hospital_ft +
parks_cnt_0p25mi +
log1p_crime_cnt_0p5,
data = na.omit(OPA_points),
method = "lm",
trControl = train_control
)
model3_cv
# Model 4: Structural + Census + Spatial + Interaction
model4_cv <- train(
sale_price ~
total_livable_area +
number_of_bedrooms +
number_of_bathrooms +
building_age +
total_area +
pop_totE +
med_hh_incE +
med_ageE +
dist_nearest_transit_ft +
dist_nearest_hospital_ft +
parks_cnt_0p25mi +
log1p_crime_cnt_0p5 +
number_of_bathrooms * wealthy_neighborhood +
int_type_tarea +
int_value_larea +
int_value_tarea +
int_larea_econd +
int_larea_icond +
int_larea_beds,
data = na.omit(OPA_points_copy),
method = "lm",
trControl = train_control
)
model4_cv
cv_results <- data.frame(
Model = c("Model 1",
"Model 2",
"Model 3",
"Model 4"),
RMSE = c(
model1_cv$results$RMSE,
model2_cv$results$RMSE,
model3_cv$results$RMSE,
model4_cv$results$RMSE
),
log_RMSE = c(
log(model1_cv$results$RMSE),
log(model2_cv$results$RMSE),
log(model3_cv$results$RMSE),
log(model4_cv$results$RMSE)
),
MAE = c(
model1_cv$results$MAE,
model2_cv$results$MAE,
model3_cv$results$MAE,
model4_cv$results$MAE
),
R_squared = c(
model1_cv$results$Rsquared,
model2_cv$results$Rsquared,
model3_cv$results$Rsquared,
model4_cv$results$Rsquared
)
)
print(cv_results)
# create model coefficient table in stargazer
models_list <- list(model1, model2, model3, model4)
models_summary_table <- stargazer(models_list, type = "text", style = "default")
# plot predicted vs actual value plots
ggplot(model1_cv$pred, aes(x = obs, y = pred)) +
geom_point(alpha = 0.3) +
geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
scale_x_continuous(labels = dollar_format()) +
scale_y_continuous(labels = dollar_format()) +
labs(
title = "Model 1 Cross-Validation: Predicted vs. Actual Sale Price",
x = "Actual Sale Price",
y = "Predicted Sale Price"
) +
theme_minimal()
ggplot(model2_cv$pred, aes(x = obs, y = pred)) +
geom_point(alpha = 0.3) +
geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
scale_x_continuous(labels = dollar_format()) +
scale_y_continuous(labels = dollar_format()) +
labs(
title = "Model 2 Cross-Validation: Predicted vs. Actual Sale Price",
x = "Actual Sale Price",
y = "Predicted Sale Price"
) +
theme_minimal()
ggplot(model3_cv$pred, aes(x = obs, y = pred)) +
geom_point(alpha = 0.3) +
geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
scale_x_continuous(labels = dollar_format()) +
scale_y_continuous(labels = dollar_format()) +
labs(
title = "Model 3 Cross-Validation: Predicted vs. Actual Sale Price",
x = "Actual Sale Price",
y = "Predicted Sale Price"
) +
theme_minimal()
ggplot(model4_cv$pred, aes(x = obs, y = pred)) +
geom_point(alpha = 0.3) +
geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
scale_x_continuous(labels = dollar_format()) +
scale_y_continuous(labels = dollar_format()) +
labs(
title = "Model 4 Cross-Validation: Predicted vs. Actual Sale Price",
x = "Actual Sale Price",
y = "Predicted Sale Price"
) +
theme_minimal()
# create diagnostic plots
plot(model4)
OPA_points_clean <- na.omit(OPA_points_copy)
OPA_points_clean$residuals <- residuals(model4)
points_with_nhoods <- st_join(OPA_points_clean, nhoods)
avg_residuals_by_nhood <- points_with_nhoods %>%
st_drop_geometry() %>%
group_by(NAME) %>%
summarise(
avg_residual = mean(residuals, na.rm = TRUE),
n_properties = n()
)
nhoods_with_residuals <- nhoods %>%
left_join(avg_residuals_by_nhood, by = "NAME")
ggplot(nhoods_with_residuals) +
geom_sf(aes(fill = avg_residual), color = "white", size = 0.15) +
scale_fill_gradient2(
low = "red",
mid = "gray90",
high = "purple",
midpoint = 0,
name = "Avg Residual ($)",
limits = c(-1, 1) * max(abs(nhoods_with_residuals$avg_residual), na.rm = TRUE)
) +
theme_minimal() +
labs(
title = "Model Performance by Neighborhood",
subtitle = "Purple = Over-prediction \nRed = Under-prediction"
) +
theme(
axis.text = element_blank(),
axis.ticks = element_blank(),
panel.grid = element_blank(),
plot.title = element_text(face = "bold", size = 15),
plot.subtitle = element_text(size = 10),
legend.position = "right",
)
